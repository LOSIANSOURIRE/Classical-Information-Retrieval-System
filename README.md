üìö Classical Information Retrieval System

This project implements a comprehensive Information Retrieval (IR) system from scratch, designed to parse, process, and rank documents against user queries. Developed and evaluated using the Cranfield dataset, this system serves as a benchmark for exploring classical and advanced IR techniques.

üöÄ Features

End-to-End IR Pipeline: Complete text-processing tools, including:

Sentence Segmentation

Tokenization

Stopword Removal

Inflection Reduction (Stemming/Lemmatization)

Multiple Retrieval Models:

Vector Space Model (VSM): TF-IDF + Cosine Similarity.

Latent Semantic Analysis (LSA): Uses SVD to reveal hidden semantic structure.

Case Retrieval Net (CRN): Expands queries using a term-term similarity matrix.

Typo Correction: Corrects spelling errors in user queries using edit distance and Jaccard similarity.

Autocomplete: Suggests query completions based on corpus vocabulary.

Modular Architecture: Clean, reusable, and extendable module structure.

Comprehensive Evaluation: Implements standard IR metrics:

Precision@k

Mean Average Precision (MAP)

üèõÔ∏è System Architecture

The system is orchestrated via main.py and follows a modular, pipelined structure.

1. Preprocessing Pipeline
Input Text
  ‚Üí Sentence Segmentation
  ‚Üí Tokenization
  ‚Üí Stopword Removal
  ‚Üí Inflection Reduction
  ‚Üí Processed Tokens

2. Query Handling

Same preprocessing pipeline as documents

Autocomplete: Suggests terms as user types

Typo Correction: Suggests corrections post-query

3. Retrieval and Ranking

Processed query is passed to a retrieval model:

VSM

LSA

CRN

Returns an ordered list of documents ranked by relevance.

4. Evaluation

Compares ranked documents against ground truth (qrels file):

Calculates MAP and Precision@k

üõ†Ô∏è Core Modules
Module	Description
main.py	Entry point. Manages CLI, preprocessing, retrieval, and evaluation.
util.py	Manages imports and downloads required NLTK data.
Preprocessing

sentenceSegmentation.py: Naive or NLTK-based sentence splitting.

tokenization.py: Naive or Penn TreeBank tokenization.

stopwordRemoval.py: NLTK-based stopword removal.

stopwordRemoval_bottom_up.py: IDF-based stopword detection.

inflectionReduction.py: Uses Porter Stemmer and WordNet Lemmatizer.

Retrieval Models

informationRetrieval.py: Implements VSM with TF-IDF and cosine similarity.

LSA.py: Implements LSA using SVD on the term-document matrix.

CRN.py: Implements Case Retrieval Net for query expansion.

Query Correction & Autocomplete

Vocabulary.py: Uses bigrams and Jaccard similarity to suggest corrections.

Edit_distance.py: Implements Levenshtein distance to rank candidates.

Autocomplete is integrated in the interactive query input loop in main.py.

Evaluation

evaluation.py: Computes:

Precision@k

Recall

Mean Average Precision (MAP)

‚öôÔ∏è Setup and Installation
‚úÖ Prerequisites

Python 3.x

pip

üì¶ Clone the Repository
git clone https://github.com/your-username/your-repository-name.git
cd your-repository-name

üì• Install Dependencies

The system uses NLTK, NumPy, SciPy, and scikit-learn. Use pip to install dependencies:

pip install nltk numpy scipy scikit-learn


The first run will automatically download NLTK models via util.py.

üíª Usage

All functionality is controlled via main.py.

1. Evaluate on Cranfield Dataset
python main.py -dataset /path/to/cranfield/


Outputs MAP and other metrics.

Saves results in the output/ directory.

2. Interactive Custom Query
python main.py -dataset /path/to/cranfield/ -custom


Prompts user for input.

Offers autocomplete suggestions.

Applies typo correction if needed.

Returns top 5 most relevant document IDs.

üßæ Command-Line Arguments
Argument	Description
-dataset	Path to dataset folder (e.g., cranfield/)
-out_folder	Output folder path (default: output/)
-segmenter	Sentence segmenter: naive or punkt
-tokenizer	Tokenizer type: naive or ptb
-custom	Enables custom query mode
